MacBook-Pro-de-Baring:Spark_Unit_20 rafaelbaring$ spark-submit spark_post_sales_unit_20.py
21/04/19 18:32:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
21/04/19 18:32:52 INFO SparkContext: Running Spark version 3.1.1
21/04/19 18:32:52 INFO ResourceUtils: ==============================================================
21/04/19 18:32:52 INFO ResourceUtils: No custom resources configured for spark.driver.
21/04/19 18:32:52 INFO ResourceUtils: ==============================================================
21/04/19 18:32:52 INFO SparkContext: Submitted application: optimize_post_sale
21/04/19 18:32:52 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
21/04/19 18:32:52 INFO ResourceProfile: Limiting resource is cpu
21/04/19 18:32:52 INFO ResourceProfileManager: Added ResourceProfile id: 0
21/04/19 18:32:52 INFO SecurityManager: Changing view acls to: rafaelbaring
21/04/19 18:32:52 INFO SecurityManager: Changing modify acls to: rafaelbaring
21/04/19 18:32:52 INFO SecurityManager: Changing view acls groups to:
21/04/19 18:32:52 INFO SecurityManager: Changing modify acls groups to:
21/04/19 18:32:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(rafaelbaring); groups with view permissions: Set(); users  with modify permissions: Set(rafaelbaring); groups with modify permissions: Set()
21/04/19 18:32:52 INFO Utils: Successfully started service 'sparkDriver' on port 58700.
21/04/19 18:32:52 INFO SparkEnv: Registering MapOutputTracker
21/04/19 18:32:52 INFO SparkEnv: Registering BlockManagerMaster
21/04/19 18:32:52 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/04/19 18:32:52 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/04/19 18:32:52 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/04/19 18:32:52 INFO DiskBlockManager: Created local directory at /private/var/folders/5r/3sm4h2510w35tymf60289_v00000gn/T/blockmgr-1ec9ab26-bd0a-4920-81f2-a363a06828e9
21/04/19 18:32:52 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
21/04/19 18:32:53 INFO SparkEnv: Registering OutputCommitCoordinator
21/04/19 18:32:53 INFO Utils: Successfully started service 'SparkUI' on port 4040.
21/04/19 18:32:53 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.166:4040
21/04/19 18:32:53 INFO Executor: Starting executor ID driver on host 192.168.0.166
21/04/19 18:32:53 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58701.
21/04/19 18:32:53 INFO NettyBlockTransferService: Server created on 192.168.0.166:58701
21/04/19 18:32:53 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/04/19 18:32:53 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.166, 58701, None)
21/04/19 18:32:53 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.166:58701 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.0.166, 58701, None)
21/04/19 18:32:53 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.166, 58701, None)
21/04/19 18:32:53 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.166, 58701, None)
21/04/19 18:32:54 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 293.9 KiB, free 366.0 MiB)
21/04/19 18:32:54 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.0 KiB, free 366.0 MiB)
21/04/19 18:32:54 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.166:58701 (size: 27.0 KiB, free: 366.3 MiB)
21/04/19 18:32:54 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0
21/04/19 18:32:55 INFO FileInputFormat: Total input files to process : 1
21/04/19 18:32:55 INFO SparkContext: Starting job: collect at /Users/rafaelbaring/Documents/GitHub/SpringBoard/Spark_Unit_20/spark_post_sales_unit_20.py:44
21/04/19 18:32:55 INFO DAGScheduler: Registering RDD 3 (reduceByKey at /Users/rafaelbaring/Documents/GitHub/SpringBoard/Spark_Unit_20/spark_post_sales_unit_20.py:41) as input to shuffle 0
21/04/19 18:32:55 INFO DAGScheduler: Got job 0 (collect at /Users/rafaelbaring/Documents/GitHub/SpringBoard/Spark_Unit_20/spark_post_sales_unit_20.py:44) with 1 output partitions
21/04/19 18:32:55 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /Users/rafaelbaring/Documents/GitHub/SpringBoard/Spark_Unit_20/spark_post_sales_unit_20.py:44)
21/04/19 18:32:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
21/04/19 18:32:55 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
21/04/19 18:32:55 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /Users/rafaelbaring/Documents/GitHub/SpringBoard/Spark_Unit_20/spark_post_sales_unit_20.py:41), which has no missing parents
21/04/19 18:32:55 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.4 KiB, free 366.0 MiB)
21/04/19 18:32:55 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 366.0 MiB)
21/04/19 18:32:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.166:58701 (size: 7.4 KiB, free: 366.3 MiB)
21/04/19 18:32:55 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1383
21/04/19 18:32:55 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /Users/rafaelbaring/Documents/GitHub/SpringBoard/Spark_Unit_20/spark_post_sales_unit_20.py:41) (first 15 tasks are for partitions Vector(0))
21/04/19 18:32:55 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
21/04/19 18:32:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.166, executor driver, partition 0, PROCESS_LOCAL, 4534 bytes) taskResourceAssignments Map()
21/04/19 18:32:55 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
21/04/19 18:32:55 INFO HadoopRDD: Input split: file:/Users/rafaelbaring/Documents/GitHub/SpringBoard/Spark_Unit_20/data.csv:0+993
21/04/19 18:32:56 INFO PythonRunner: Times: total = 515, boot = 500, init = 14, finish = 1
21/04/19 18:32:56 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1655 bytes result sent to driver
21/04/19 18:32:56 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1398 ms on 192.168.0.166 (executor driver) (1/1)
21/04/19 18:32:56 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
21/04/19 18:32:56 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 58702
21/04/19 18:32:56 INFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at /Users/rafaelbaring/Documents/GitHub/SpringBoard/Spark_Unit_20/spark_post_sales_unit_20.py:41) finished in 1.564 s
21/04/19 18:32:56 INFO DAGScheduler: looking for newly runnable stages
21/04/19 18:32:56 INFO DAGScheduler: running: Set()
21/04/19 18:32:56 INFO DAGScheduler: waiting: Set(ResultStage 1)
21/04/19 18:32:56 INFO DAGScheduler: failed: Set()
21/04/19 18:32:56 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at collect at /Users/rafaelbaring/Documents/GitHub/SpringBoard/Spark_Unit_20/spark_post_sales_unit_20.py:44), which has no missing parents
21/04/19 18:32:56 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.0 KiB, free 366.0 MiB)
21/04/19 18:32:56 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.0 MiB)
21/04/19 18:32:56 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.166:58701 (size: 5.8 KiB, free: 366.3 MiB)
21/04/19 18:32:56 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1383
21/04/19 18:32:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[6] at collect at /Users/rafaelbaring/Documents/GitHub/SpringBoard/Spark_Unit_20/spark_post_sales_unit_20.py:44) (first 15 tasks are for partitions Vector(0))
21/04/19 18:32:56 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
21/04/19 18:32:56 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.166, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
21/04/19 18:32:56 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
21/04/19 18:32:56 INFO ShuffleBlockFetcherIterator: Getting 1 (304.0 B) non-empty blocks including 1 (304.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/04/19 18:32:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
21/04/19 18:32:56 INFO PythonRunner: Times: total = 9, boot = -581, init = 590, finish = 0
21/04/19 18:32:57 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1733 bytes result sent to driver
21/04/19 18:32:57 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 101 ms on 192.168.0.166 (executor driver) (1/1)
21/04/19 18:32:57 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
21/04/19 18:32:57 INFO DAGScheduler: ResultStage 1 (collect at /Users/rafaelbaring/Documents/GitHub/SpringBoard/Spark_Unit_20/spark_post_sales_unit_20.py:44) finished in 0.122 s
21/04/19 18:32:57 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
21/04/19 18:32:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
21/04/19 18:32:57 INFO DAGScheduler: Job 0 finished: collect at /Users/rafaelbaring/Documents/GitHub/SpringBoard/Spark_Unit_20/spark_post_sales_unit_20.py:44, took 1.815807 s
21/04/19 18:32:57 INFO SparkContext: Starting job: collect at /Users/rafaelbaring/Documents/GitHub/SpringBoard/Spark_Unit_20/spark_post_sales_unit_20.py:49
21/04/19 18:32:57 INFO DAGScheduler: Registering RDD 9 (reduceByKey at /Users/rafaelbaring/Documents/GitHub/SpringBoard/Spark_Unit_20/spark_post_sales_unit_20.py:48) as input to shuffle 1
21/04/19 18:32:57 INFO DAGScheduler: Got job 1 (collect at /Users/rafaelbaring/Documents/GitHub/SpringBoard/Spark_Unit_20/spark_post_sales_unit_20.py:49) with 1 output partitions
21/04/19 18:32:57 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /Users/rafaelbaring/Documents/GitHub/SpringBoard/Spark_Unit_20/spark_post_sales_unit_20.py:49)
21/04/19 18:32:57 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
21/04/19 18:32:57 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
21/04/19 18:32:57 INFO DAGScheduler: Submitting ShuffleMapStage 2 (PairwiseRDD[9] at reduceByKey at /Users/rafaelbaring/Documents/GitHub/SpringBoard/Spark_Unit_20/spark_post_sales_unit_20.py:48), which has no missing parents
21/04/19 18:32:57 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 8.9 KiB, free 365.9 MiB)
21/04/19 18:32:57 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 365.9 MiB)
21/04/19 18:32:57 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.166:58701 (size: 5.7 KiB, free: 366.3 MiB)
21/04/19 18:32:57 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1383
21/04/19 18:32:57 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (PairwiseRDD[9] at reduceByKey at /Users/rafaelbaring/Documents/GitHub/SpringBoard/Spark_Unit_20/spark_post_sales_unit_20.py:48) (first 15 tasks are for partitions Vector(0))
21/04/19 18:32:57 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
21/04/19 18:32:57 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.166, executor driver, partition 0, PROCESS_LOCAL, 4547 bytes) taskResourceAssignments Map()
21/04/19 18:32:57 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
21/04/19 18:32:57 INFO PythonRunner: Times: total = 3, boot = -129, init = 132, finish = 0
21/04/19 18:32:57 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1483 bytes result sent to driver
21/04/19 18:32:57 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 46 ms on 192.168.0.166 (executor driver) (1/1)
21/04/19 18:32:57 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
21/04/19 18:32:57 INFO DAGScheduler: ShuffleMapStage 2 (reduceByKey at /Users/rafaelbaring/Documents/GitHub/SpringBoard/Spark_Unit_20/spark_post_sales_unit_20.py:48) finished in 0.063 s
21/04/19 18:32:57 INFO DAGScheduler: looking for newly runnable stages
21/04/19 18:32:57 INFO DAGScheduler: running: Set()
21/04/19 18:32:57 INFO DAGScheduler: waiting: Set(ResultStage 3)
21/04/19 18:32:57 INFO DAGScheduler: failed: Set()
21/04/19 18:32:57 INFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[12] at collect at /Users/rafaelbaring/Documents/GitHub/SpringBoard/Spark_Unit_20/spark_post_sales_unit_20.py:49), which has no missing parents
21/04/19 18:32:57 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 8.6 KiB, free 365.9 MiB)
21/04/19 18:32:57 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 365.9 MiB)
21/04/19 18:32:57 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.166:58701 (size: 5.1 KiB, free: 366.3 MiB)
21/04/19 18:32:57 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1383
21/04/19 18:32:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (PythonRDD[12] at collect at /Users/rafaelbaring/Documents/GitHub/SpringBoard/Spark_Unit_20/spark_post_sales_unit_20.py:49) (first 15 tasks are for partitions Vector(0))
21/04/19 18:32:57 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
21/04/19 18:32:57 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (192.168.0.166, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()
21/04/19 18:32:57 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
21/04/19 18:32:57 INFO ShuffleBlockFetcherIterator: Getting 1 (106.0 B) non-empty blocks including 1 (106.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/04/19 18:32:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
21/04/19 18:32:57 INFO PythonRunner: Times: total = 3, boot = -57, init = 59, finish = 1
21/04/19 18:32:57 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1677 bytes result sent to driver
21/04/19 18:32:57 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 33 ms on 192.168.0.166 (executor driver) (1/1)
21/04/19 18:32:57 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
21/04/19 18:32:57 INFO DAGScheduler: ResultStage 3 (collect at /Users/rafaelbaring/Documents/GitHub/SpringBoard/Spark_Unit_20/spark_post_sales_unit_20.py:49) finished in 0.048 s
21/04/19 18:32:57 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
21/04/19 18:32:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
21/04/19 18:32:57 INFO DAGScheduler: Job 1 finished: collect at /Users/rafaelbaring/Documents/GitHub/SpringBoard/Spark_Unit_20/spark_post_sales_unit_20.py:49, took 0.129792 s
('Nissan', 1)
('Mercedes', 3)
('Toyota', 0)
21/04/19 18:32:57 INFO SparkContext: Invoking stop() from shutdown hook
21/04/19 18:32:57 INFO SparkUI: Stopped Spark web UI at http://192.168.0.166:4040
21/04/19 18:32:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/04/19 18:32:57 INFO MemoryStore: MemoryStore cleared
21/04/19 18:32:57 INFO BlockManager: BlockManager stopped
21/04/19 18:32:57 INFO BlockManagerMaster: BlockManagerMaster stopped
21/04/19 18:32:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/04/19 18:32:57 INFO SparkContext: Successfully stopped SparkContext
21/04/19 18:32:57 INFO ShutdownHookManager: Shutdown hook called
21/04/19 18:32:57 INFO ShutdownHookManager: Deleting directory /private/var/folders/5r/3sm4h2510w35tymf60289_v00000gn/T/spark-582fff2e-79c2-4cfb-9159-e5a0b52bcb4f/pyspark-fcf043a3-5e8e-440a-b280-b0acfb9aecd8
21/04/19 18:32:57 INFO ShutdownHookManager: Deleting directory /private/var/folders/5r/3sm4h2510w35tymf60289_v00000gn/T/spark-641ff685-18e7-482e-9ead-2b9e4a56211b
21/04/19 18:32:57 INFO ShutdownHookManager: Deleting directory /private/var/folders/5r/3sm4h2510w35tymf60289_v00000gn/T/spark-582fff2e-79c2-4cfb-9159-e5a0b52bcb4f
MacBook-Pro-de-Baring:Spark_Unit_20 rafaelbaring$
